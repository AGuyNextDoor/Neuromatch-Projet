{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopie von Dataset Generation_Pascal.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Sx748h0FiS0","executionInfo":{"status":"ok","timestamp":1658908750179,"user_tz":-120,"elapsed":2585,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}},"outputId":"656b9c85-8324-416a-e3eb-81b3f441acf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["!ls \"gdrive/MyDrive/Neuromatch Project\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H60S8KWcpvOD","executionInfo":{"status":"ok","timestamp":1658908750181,"user_tz":-120,"elapsed":25,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}},"outputId":"be9e8ad9-b378-4d20-bca5-d72382d7e141"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":[" Animation\t 'Group 2 : Abstract.gdoc'  'Project Abstract.gdoc'\n","'Colab Drafts'\t  LICENSE\t\t     README.md\n","'Colab Project'   LINKS.gdoc\t\t     Template_ECoG_memory_nback\n"," Datasets\t 'Metadata Notes.gdoc'\t    'Untitled spreadsheet.gsheet'\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"hTNGGnKBK8Wf","executionInfo":{"status":"ok","timestamp":1658908751493,"user_tz":-120,"elapsed":1325,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# @title Data retrieval\n","import os, requests\n","\n","path = \"./gdrive/MyDrive/Neuromatch Project/Datasets/\"\n","name = 'memory_nback.npz'\n","fname = path+name\n","url = \"https://osf.io/xfc7e/download\"\n","\n","if not os.path.isfile(fname):\n","  try:\n","    r = requests.get(url)\n","  except requests.ConnectionError:\n","    print(\"!!! Failed to download data !!!\")\n","  else:\n","    if r.status_code != requests.codes.ok:\n","      print(\"!!! Failed to download data !!!\")\n","    else:\n","      with open(fname, \"wb\") as fid:\n","        fid.write(r.content)"],"metadata":{"id":"-eDPNW-TF_rz","executionInfo":{"status":"ok","timestamp":1658908774173,"user_tz":-120,"elapsed":22686,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# @title Data loading\n","import numpy as np\n","\n","alldat = np.load(fname, allow_pickle=True)['dat']\n","\n","# Select just one of the recordings here. This is subject 1, block 1.\n","dat = alldat[1][1]\n","\n","print(dat.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-w-h_wT8HM7y","executionInfo":{"status":"ok","timestamp":1658908775776,"user_tz":-120,"elapsed":1613,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}},"outputId":"d45983ca-cfba-4815-ec2a-4e59d80549f9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['V', 't_off', 'locs', 'srate', 'scale_uv', 't_on', 'target', 'stim_id', 'response', 'rt', 'expinfo', 'hemisphere', 'lobe', 'gyrus', 'Brodmann_Area'])\n"]}]},{"cell_type":"code","source":["# compute spectral power above 50Hz and low-pass below 10Hz\n","# power is always positive, so we normalize it by its average\n","\n","from scipy import signal\n","def voltage_from_data(complete_dataset = alldat, subject = 1, experiment = 1, upper_limit = 50, lower_limit = 10):\n","\n","  # pick subject 1 and experiment 1\n","  dat = complete_dataset[subject][experiment]\n","  V = dat['V'].astype('float32') # always convert the voltage data to float32!\n","\n","  # if np.isnan(lower_limit) == False and np.isnan(upper_limit) == False :\n","  #   b, a = signal.butter(3, [lower_limit], btype='high', fs=1000)\n","\n","\n","  # high-pass filter above `lower-limit` Hz\n","  if np.isnan(lower_limit) == False : \n","    b, a = signal.butter(3, [lower_limit], btype='high', fs=1000)\n","    V = signal.filtfilt(b, a, V, 0)\n","\n","  # low-pass filter above `upper-limit` Hz\n","  # if np.isnan(upper_limit) == False :\n","  else :\n","    b, a = signal.butter(3, [upper_limit], btype='low', fs=1000)\n","    V = signal.filtfilt(b, a, V, 0)\n","\n","  # compute smooth envelope of this signal = approx power\n","  V = np.abs(V)**2\n","  b, a = signal.butter(3, [10], btype='low', fs=1000)\n","  V = signal.filtfilt(b, a, V, 0)\n","\n","  # compute smooth envelope of this signal = approx power\n","  # normalize each channel so its mean power is 1\n","  V = V/V.mean(0)\n","\n","  return V\n","\n","\n","def get_epochs(complete_dataset = alldat, subject = 1, experiment = 1, upper_limit = 50, lower_limit = 10, time_window = [-400, 1600], per_trial = True):\n","  V = voltage_from_data(alldat, subject, experiment, upper_limit, lower_limit)\n","  dat = complete_dataset[subject, experiment]\n","\n","  # divide into trials and average\n","  nt, nchan = V.shape\n","  nstim = len(dat['t_on'])\n","\n","  # Use a timerange from time_wondow[0] to time_window[1] after the stimulus onset\n","\n","  trange = np.arange(time_window[0], time_window[1])\n","  calc_trange = time_window[1] - time_window[0]\n","  ts = dat['t_on'][:, np.newaxis] + trange\n","  if per_trial : \n","    V_epochs = np.reshape(V[ts, :], (nstim, calc_trange, nchan))\n","  else :\n","    V_epochs = np.reshape(V[ts, :], (nstim * calc_trange, nchan))\n","\n","  return V_epochs\n","\n","def get_resp_base(mean = False, complete_dataset = alldat, subject = 1, experiment = 1):\n","  V = voltage_from_data(complete_dataset, subject, experiment)\n","  dat = complete_dataset[subject][experiment]\n","\n","    # divide into trials and average\n","  nt, nchan = V.shape\n","  nstim = len(dat['t_on'])\n","\n","  # use a timerange from 400ms before to 1600ms after the stimulus onset\n","\n","  trange = np.arange(-400, 1600)\n","  ts = dat['t_on'][:, np.newaxis] + trange\n","  V_epochs = np.reshape(V[ts, :], (nstim, 2000, nchan))\n","\n","  if mean :\n","    V_resp = (V_epochs[dat['response'] == 1]).mean(0)\n","    V_base = (V_epochs[dat['response'] == 0]).mean(0)\n","    return V_resp, V_base, trange\n","\n","  else :\n","    V_resp_tri = V_epochs[dat['response'] == 1]\n","    V_base_tri = V_epochs[dat['response'] == 0]\n","    return V_resp_tri, V_base_tri, trange\n","\n","\n"],"metadata":{"id":"nS2QXoOLHS08","executionInfo":{"status":"ok","timestamp":1658908775779,"user_tz":-120,"elapsed":15,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["V_resp, V_base, trange = get_resp_base()\n","V_resp_mean, V_base_mean, trange_mean = get_resp_base(True)"],"metadata":{"id":"ORn8qmFUHVAa","executionInfo":{"status":"ok","timestamp":1658908781318,"user_tz":-120,"elapsed":5550,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## PCA"],"metadata":{"id":"mMhkQ2A4H0Zu"}},{"cell_type":"code","source":["def get_variance_explained(evals):\n","  # Cumulatively sum the eigenvalues\n","  csum = np.cumsum(evals)\n","\n","  # Normalize by the sum of eigenvalues\n","  variance_explained = csum/np.sum(evals)\n","\n","  return variance_explained\n","\n","def plot_variance_explained(variance_explained, axis = False):\n","\n","  plt.figure()\n","  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n","           '--k')\n","  plt.xlabel('Cummulated Number of P_components from channels')\n","  plt.ylabel('Variance explained')\n","  if axis : \n","    plt.ylim([0, 1])\n","  plt.show()\n","\n","def sort_evals_descending(evals, evectors):\n","\n","  index = np.flip(np.argsort(evals))\n","  evals = evals[index]\n","  evectors = evectors[:, index]\n","  if evals.shape[0] == 2:\n","    if np.arccos(np.matmul(evectors[:, 0],1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n","      evectors[:, 0] = -evectors[:, 0]\n","    if np.arccos(np.matmul(evectors[:, 1],1 / np.sqrt(2)*np.array([-1, 1]))) > np.pi / 2:\n","      evectors[:, 1] = -evectors[:, 1]\n","\n","  return evals, evectors\n","\n","def plot_PCA_weights(weights):\n","\n","  vmax = np.max(np.abs(weights))\n","\n","  fig, ax = plt.subplots()\n","  cmap = plt.cm.get_cmap('seismic')\n","  plt.imshow(np.real(weights), cmap=cmap)\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  plt.clim(-vmax, vmax)\n","  plt.colorbar(ticks=np.arange(-vmax, vmax, 0.1))\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  plt.show()\n","\n","def plot_V_activity(activity):\n","\n","  # vmax = np.max(activity)\n","  # vmin = np.min(activity)\n","  vmax = 3\n","  vmin = 0\n","\n","  fig, ax = plt.subplots()\n","  cmap = plt.cm.get_cmap('plasma')\n","  plt.imshow(np.real(activity), cmap=cmap)\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  plt.clim(vmin, vmax)\n","  plt.colorbar(ticks=np.arange(vmin, vmax, 0.1))\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  plt.show()"],"metadata":{"id":"wneuXoeBHcTW","executionInfo":{"status":"ok","timestamp":1658908781320,"user_tz":-120,"elapsed":58,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def pca(X):\n","  \n","  X_mean = X - np.mean(X, 0)\n","  X_COV = np.cov(X_mean.T)\n","\n","  evals, evectors = np.linalg.eigh(X_COV)\n","\n","  evals, evectors = sort_evals_descending(evals, evectors)\n","\n","  score = X @ evectors\n","\n","  # Calculate the variance explained\n","  variance_explained = get_variance_explained(evals)\n","\n","  return variance_explained, score, evals, evectors"],"metadata":{"id":"yVybT7EzH2jj","executionInfo":{"status":"ok","timestamp":1658908781321,"user_tz":-120,"elapsed":56,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Calculate RMS of voltages\n","def RMS(time_series):\n","  return np.sqrt(np.mean((np.square(time_series))))"],"metadata":{"id":"yOi1AkxmhtPX","executionInfo":{"status":"ok","timestamp":1658908781323,"user_tz":-120,"elapsed":53,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","import random"],"metadata":{"id":"VkDf3KPwlOUQ","executionInfo":{"status":"ok","timestamp":1658910822469,"user_tz":-120,"elapsed":636,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["test_size = 0.2                    # Value between 0 and 1\n","version_1_or_2 = False             # True is version 1 with only channels as columns, Flase is version 2 with channels and time steps in columns\n","pca_reduction = True # False               # Are channels reduced by number of pca components?\n","data_processing = ''            # string describing the data procesing procedure before modelling, 'rms', 'derivative', 'raw'\n","trials_are_meaned = False          # Are the trials meaned?\n","num_of_principal_comp = 20         # If pca_reduction, choose how many principal components are kept    \n","frequency_range = (50, np.nan)     # Chooosing the frequencies kept\n","time_window = [-400, 1600]         # Choosing the time window around the stimulus\n","time_bin_size = 10                     # Size of the time bin\n","subject_list = [0, 1]                  # Subject number, has to be in array even if only one subject is wanted\n","experiment_list = [1, 2, 3]            # Experiment number, has to be in an array if only one subject is wanted"],"metadata":{"id":"E1DpkWuZJyRQ","executionInfo":{"status":"ok","timestamp":1658910823766,"user_tz":-120,"elapsed":22,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def dataset_generation(version_1_or_2, test_size, data_processing, pca_reduction, num_of_principal_comp, frequency_range, time_window, time_bin_size, subject_list, experiment_list ):\n","\n","  all_data = []\n","  all_scores = []\n","  all_responses = []\n","\n","  lower_limit, upper_limit = frequency_range\n","\n","  # Construct/Concat the dataset\n","\n","  for subject in subject_list : \n","    for experiment in experiment_list : \n","\n","      # complete_dataset = alldat, subject = 1, experiment = 1, upper_limit = 50, lower_limit = 10'\n","      epochs = get_epochs(alldat, subject, experiment, upper_limit, lower_limit, time_window, True) # do some more testing\n","      \n","      V_epochs = []\n","      index = 0\n","      for epoch in epochs : \n","        df = pd.DataFrame(epoch)\n","        \n","        # Binning\n","        df = pd.DataFrame(df.groupby(df.index // time_bin_size).mean()) \n","            # Keep in mind the downsample, also the function `resample` in pandas but needs to add the dates as index\n","            # Same function `resample` and `decimate` in scipy could work\n","        \n","        if data_processing=='rms':\n","          df = pd.DataFrame(df.apply(RMS, axis=0)).transpose()\n","\n","        elif data_processing=='derivative':\n","          df = df.apply(np.diff, axis=0)\n","\n","        # Creating index values per trial, ex \n","        value = (10000*(subject + 1) + 1000*(experiment+1) + index) \n","        df[\"trial_id\"] = value\n","\n","        # Assigning target values\n","        df[\"target\"] = alldat[subject][experiment]['response'][index]\n","        V_epochs.append(df)\n","        index+=1  \n","     \n","      \n","      V_epochs = pd.concat(V_epochs)\n","\n","      all_data.append(V_epochs)\n","\n","  # Stacking all the experiments from all subjects in one\n","  all_data_df = pd.concat(all_data)\n","  print(\"all_data shape : \", all_data_df.shape)\n","\n","  # Take one value per trial \n","  all_data_grouped = all_data_df.groupby(by= [\"trial_id\"]).tail(1).set_index(\"trial_id\")\n","  X_train_index, X_test_index, y_train_index, y_test_index = train_test_split(all_data_grouped.drop([\"target\"], axis = 1), all_data_grouped[\"target\"], test_size=test_size, random_state=42, stratify=all_data_grouped[\"target\"])\n","    \n","  train_data = all_data_df[all_data_df[\"trial_id\"].apply(lambda x : x in X_train_index.index.values)].set_index(\"trial_id\")\n","  X_train = train_data.drop([\"target\"], axis = 1)\n","  y_train = train_data[\"target\"]\n","\n","  test_data = all_data_df[all_data_df[\"trial_id\"].apply(lambda x : x in X_test_index.index.values)].set_index(\"trial_id\")\n","  X_test = test_data.drop([\"target\"], axis = 1)\n","  y_test = test_data[\"target\"]\n","\n","  # Training and on test sets\n","  if pca_reduction : \n","\n","    # train_extra_col = X_train[extra_col]\n","    train_PCA = PCA(n_components = num_of_principal_comp).fit(X_train) # To be checked if that is the best procedure\n","    X_train = pd.DataFrame(train_PCA.transform(X_train), index = X_train.index)\n","    X_test = pd.DataFrame(train_PCA.transform(X_test), index = X_test.index)\n","\n","  # \n","  if version_1_or_2 == False :\n","\n","    X_train = X_train.reset_index().groupby(by = [\"trial_id\"]).apply(lambda g : pd.DataFrame([g.drop([\"trial_id\"], axis=1).to_numpy().flatten()])).reset_index().drop([\"level_1\",\"trial_id\"], axis = 1)# numpy to flatten - fastest way to flatten your matrix\n","    y_train = y_train.reset_index().groupby(\"trial_id\").tail(1).drop([\"trial_id\"], axis = 1)\n","\n","    X_test = X_test.reset_index().groupby(by = [\"trial_id\"]).apply(lambda g : pd.DataFrame([g.drop([\"trial_id\"], axis=1).to_numpy().flatten()])).reset_index().drop([\"level_1\", \"trial_id\"], axis = 1)\n","    y_test = y_test.reset_index().groupby(\"trial_id\").tail(1).drop([\"trial_id\"], axis = 1)\n","\n","  return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = dataset_generation(version_1_or_2, test_size, data_processing, pca_reduction, num_of_principal_comp, frequency_range, time_window, time_bin_size, subject_list, experiment_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubtHwOEKQKzp","executionInfo":{"status":"ok","timestamp":1658910910315,"user_tz":-120,"elapsed":86567,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}},"outputId":"6ffff239-697f-4328-f427-1310647df6e9"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["all_data shape :  (120000, 66)\n"]}]},{"cell_type":"code","source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mP3cdhLEwVWr","executionInfo":{"status":"ok","timestamp":1658910910320,"user_tz":-120,"elapsed":24,"user":{"displayName":"Pascal Klein","userId":"07036662126354796595"}},"outputId":"1896a981-ae95-4a8d-f444-11c736e3687c"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((480, 4000), (120, 4000), (480, 1), (120, 1))"]},"metadata":{},"execution_count":20}]}]}